# 数据挖掘读书笔记-william
数据挖掘是在大型数据库中，自动地发现有用信息的过程。用来探查大型数据库，发现先前未知的有用模式，预测未来观测接结果。

并非所有的信息发现任务都是数据挖掘，上网查找个别记录只能称为信息检索(information retrieval)

数据挖掘是数据库中知识发现(knowledge discovery in database, KDD)不可缺少的一部分。
整个KDD过程为： 输入数据→数据预处(特征选择、维归约、规范化、选择数据集)→数据挖掘→后处理(模式过滤、可视化、模式表示）→信息。

数据预处理(reprocessing)的目的是将未加工的输入数据转化为适合分析的形式，涉及融合多个数据源数据，清洗数据 以消除噪声和重复的观测值，选择与当前数据挖掘任务相关的记录和特征。 最费力最耗时。

“结束循环”(closing the loop)通常指将数据挖掘成果集成到决策支持系统的过程。

数据挖掘要解决的难题：
1.可伸缩 ：数吉字节、数太字节甚至数拍字节的数据集越来越普遍，因此处理这些数据需要算法的可伸缩性，还可能需要实现新的数据结构才能有效访问每个记录。例如当要处理的数据不能放进内存时，可能需要非内存算法。使用抽样技术或并行开发和分布算法也可以提高可伸缩程度。

2.高维性： 随着维度（特征数）的增加，计算复杂性迅速增加。
3.异种数据和复杂数据
4.数据的所有权与分布：有时需要分析的数据并非分布在一个站点，这就需要分布式数据挖掘技术。分布式数据挖掘技术算法面临的挑战: 如何降低执行分布式计算所需的通信量？ 如何有效地统一从多个资源得到的数据挖掘结果？ 如何处理数据安全性问题？

数据挖掘的任务：

预测任务：其任务目标是根据其它属性，预测特定属性的值。被预测的属性称为目标变量(target variable)或因变量(dependent variable),而用来预测的属性称为说明变量(explanatory)或自变量(independent variable)。
描述任务：其目标是概括数据中潜在联系的模式(相关、趋势、聚类、轨迹和异常）
本质上，描述性数据挖掘任务通常是探查性的，并且常常需要后处理技术验证和解释结果。
四种主要数据挖掘任务：预测建模、聚类分析、关联分析、异常检测

预测建模(prodictive modeling) 涉及以说明变量函数的方式为目标变量建立模型。
有两类预测建模任务：分类(classification),用于预测离散的目标变量；回归(regression)用于预测连续的目标变量。

关联分析(association analysis) 用来发现数据中强关联特征的模式。例如：牛奶→尿布的关联模式，可以发现商品中可能存在的交叉销售的商机。

聚类分析（cluster analysis) 旨在发现紧密相关的观测值组群，使得与属于不同簇的观测值相比，属于同一簇的观测值相互之间尽可能类似。

异常检测（anomaly detection) 的任务是识别其特征显著不同于其它数据的观测值。这样的观测值称为异常点(anomaly)或离群点(outlier)。异常检测的目标是发现真正的异常点，而避免将异常点当作正常的对象。

2.属性是对象的性质或特性，它因对象而异或随时间而变化。
测量标度是将数值或符号值与对象的属性相关联的规划(函数)。

属性的性质不必与用来度量它的值的性质相同。比如一组线段 和两组数字，其中一组数字能描述长度属性，另外一组只是排序，因此属性可以用一种不描述属性全部性质的方式测量。

3.一种指定属性类型的有用方法是：确定对应于用于描述属性的数值的性质。
四种属性性质：标称(nominal)、序数(ordinal)、区间(interval)、比率(ratio)。
每种属性类型拥有其上方属性类型的所有性质和操作。

标称和序数属性被称为分类的(categorical)或定性的(qualitative)。区间和比率属性被称为定量的(quantitative)或数值的(numeric)。

4.用值的个数描述属性：
（1）离散的(discrete) （2）连续的(continuous)

5.非对称的属性，只有非零值才重要的二元属性是非对称的二元属性。

数据集的三个基本分组：记录数据、基于图形的数据和有序的数据。
数据集的三个特性：维度(dimensionality)、稀疏性(sparsity)和分辨率(resolution)。

记录数据：事务数据或购物车数据(transaction data)、数据矩阵、稀疏数据矩阵。
基于图形的数据：(1)图形捕获数据对象之间的联系。（2)数据本身用图形表示。（化学结构图）
. . 带有对象之间联系的数据和具有图像对象的数据常用图形表示。
有序数据：对于某些数据，属性具有涉及时间和空间序的联系。 时序数据(sequential data)、序列数据(sequence data)、时间序列数据(time series data)、空间序列。

测量误差(measurement error):记录的值与实际值不同
数据收集错误(data collection error):遗漏数据对象或属性值，或不当地包含了其它数据对象。
噪声是测量误差的随机部分。这可能涉及值被扭曲或加入谬误对象。鲁棒算法(robust algorithm)，即在噪声干扰下也能产生可以接受的结果。
数据错误可能是更确定性现象的结果，如一组照片在同一地方出现条纹。数据这种确定性失常称为伪像(artifact)。（现象明显特别是通过图像反映出来的错误。）

精度(precision) :(同一个量的)重复测量值之间的接近程度。通常用值集合的标准差度量。
偏倚(bias):测量值与被测值之间的系统的变差。用均值与已测值之间的差度量。

通常使用更一般的术语准确率表示数据测量误差的程度。准确率(accuracy)被测量的测量值与实际值之间的接近度。其中一个重要方面是有效数字(signification digit)。

区别离群点和噪声：离群点可以是合法的数据对象和值。
处理遗漏值的策略:1.删除数据对象或属性 2.估计遗漏值 3.在分析是忽略遗漏值。

重复数据的去重复需要考虑两个问题：1、即使两个数据值相似但他们可能是两个不同对象，是合法的。 2、如果是同一个对象，那么如何处理同属性的两个值的不一致。

数据预处理

聚集(aggregation)
抽样
维归约
特征子集选择
特征创建
离散化和二元化
变量变换
简单地讲，将这些项目分成两类，即选择分析所需要的数据对象和属性以及创建/改变属性。
聚集：

数据归约使小数据集需要较少的内存和处理时间，因此可以使用开销更大的数据挖掘算法。
通过高层而不是低层的数据图，起到了范围和标度转换的作用。
对象活属性群的行为通常比单个对象或属性的行为更加稳定。
但失去了细节。
抽样：

当稀有类型有重要性时随机抽样不合适，选择分层抽样较为合理。
合适的样本容量可能很难确定，因此有时需要使用自适应(adaptive)或渐进抽样(progressive sampling),不断增加样本容量直至得到足够容量的样本。
维归约：
通过创建新属性，将一些旧属性合并在一起来降低数据集的维度。通过选择旧属性的子集得到新属性，这种维归约称为特征子集选择或特征选取。

维归约的线性代数技术：将数据从高维投影到低维空间，特别是对于连续数据。主成分分析(PCA)是一种用于连续属性的线性代数技术，它找出新的属性（主成分），这些属性是原属性的线性组合，是相互正交的，并且捕获了数据的最大变差。奇异值分解(SVD)
特征子集：
降低维度的另一种方法是仅使用特征的一个子集,即使用部分有代表性的属性。尽管这种方法看起来可能丢失信息，但是在冗余或存在不相关特征的时候并非如此。

冗余特征重复了包含一个或多个其它属性的许多或所有信息。
不相关特征包含对手头挖掘任务几乎没用的信息。
有三种标准的特征选择方法：嵌入、过滤和包装。
嵌入方法(embeded approach)
过滤方法(filter approach)
包装方法(wrapper approach)
特征选择过程可以看作四个部分组成：子集评估度量、控制新的特征子集产生搜索策略、停止搜索判断和验证过程。过滤过程和包装过程唯一的不同是选择不同的特征子集评估方法。

另一种保留或删除特征的方法是特征加权。特征越重要，所赋予的权值越大。

2.3.5 特征构造
1.特征提取(feature extraction):我们有原始图像的所有数据点，但我们需要其中的人脸特征。
2.映射数据到新的空间：有噪声图像的影响我们不变发现数据规律，通过某种变换，例如傅里叶变换使得特征得到显现。
3.特征构造：有时原始数据的特征有必要的信息，但是构造新的特征能更好的描述，比如密度。

2.3.6 离散化(discretization)和二元化binarization
1.对于关联问题，可能需要用俩个非对称的二元属性替换单个二元属性。
2.连续属性变换成分类属性需要考虑两个子任务：决定需要多少分类值，以及确定如何将连续属性值映射到这些分类值。将连续值的区间指定n-1个分割点，得到n个区间，在把每个区间的所有值映射到相同的分类值。
3.用于分类的离散方法之间的根本区别在于使用类信息(监督，supervised)还是不使用类信息(非监督，unsupervised)。

不使用类信息: 例如使用等宽(equal width)而用户自定义区间个数。这种方法可能受离群点而效果不佳。还有等频(equal frequncy)或等深(equal depth)。非监督也可以用K均值等聚类方法。
**基于熵(entropy)**是最有前途的离散方法。一种概念上的简单方法是以极大化区间纯度的方式确定分割点，直观上，区间的熵是区间纯度的度量。如果一个区间只包含一个类的值，则其熵为0并且不影响总熵。如果一个区间的值类出现的频率相等(该区间尽可能不纯)，则其熵最大。
2.3.7 变量变换
变量变换(variable transformation)是指用于所有值的变换。我们讨论两种重要的变换：简单函数变换和规范化。

在统计学中，变量变换(特别是平方根、对数和导数变换)常用来将不具有高斯正态分布的数据变换成具有高斯正态分布的数据。
使用变量变换时需要小心，因为它们改变了数据的特性。重要的是要问如下问题：需要保序吗？变换作用于所有的值，特别是负值和0吗？变换对于0和1之间的值有何特别影响？
另一个常见的变量变换类型是变量的标准化(standardization)或规范化(normalization)。
目的是使整个值的集合具有特定的性质。
均值和标准差受离群点的影响很大，因此需要修改变换，用中位数代替均值，用绝对标准差代替标准差。

2.4 相似性和相异性的度量
在许多情况下，计算出相似性和相异性就不再需要原始数据了。这种方法可以看作将数据变换到相似性(相异性)空间，然后进行分析。

邻近度(proximity) 用来表示相似性或相异性。度量方法中，相关和欧几里得距离度量适用于时间序列这样的稠密函数或二维点，Jaccard和余弦相似性度量适用于像文档这样的稀疏数据。
相似度(similarity)为非负值，并常常在0（不相似）和1（完全相似）之间取值。
相异度(dissmilarity)，距离(distance)是其同义词。
一般来说 任何单调减函数都可以用来将相异度转换到相似度（或相反）。当然也必须考虑其它因素，涉及保持意义、扰乱标度和数据分析工具的需要，但还有其它问题。
区间或比率属性的相似度通常转换为相异度。
2.4.3数据对象之间的相异度
闵可夫斯基距离(Minkowski distance),根据r值的不同得到代表不同意义的公式，但r=2时得到欧几里得距离。

满足非负性、对称性和三角不等式的测度称为度量(metric)。此外，如果三角不等式成立，则该性质可以用来提高依赖距离的技术(包括聚类)的效率。
非度量的相异度举例：集合差和时间 p57

2.4.4数据对象之间的相似度
对于相似度，三角不等式通常不成立，但是对称性和非负性通常成立。
非对称相似性度量:混淆矩阵 0和o.

2.4.5邻近性度量的例子
1.二元数据的相似性度量

两个仅包含二元属性的对象之间的相似性度量也称为相似系数(similarity coefficient),并且通常在0和1之间的取值，值为1为完全相似。
简单匹配系数（Simple Matching Coefficient,SMC)
SMC= 值匹配的属性个数/属性个数
SMC可以在一个仅包含是非题的测验中用来发现回答问题相似的学生。
jaccard系数：常常使用Jaccard系数来处理仅包含非对称的二元属性对象。
J= 匹配的个数/不涉及0-0匹配的属性个数。
广义jaccard系数
广义jaccard系数可以用于文档数据，并在二元属性情况下归约为Jaccard系数，广义Jaccard又称为Tanimoto系数，用EJ表示。
2.相关性
两个具有二元变量或连续变量的数据对象之间的相关性是对象属性之间线性联系的度量。
更准确的，两个数据对象x和y的皮尔森相关(Pearson’s correlation)系数 p61

相关度为1代表存在线性关系。

Bregman散度(Bregman divergence)，它是一簇具有共同性质的邻近函数。 Bregman散度是损失或失真函数。例如点y是原来的点，而点x是它的某个失真或近似。而损失函数的目的是度量用x近似y导致的失真或损失，xy越相似，失真或损失就越小。p62

3.邻近度计算问题：
距离度量的一个问题是当属性具有不同值域时如何处理。(“变量具有不同尺度”)

除值域不同外，当某些属性之间还相关时，如何计算距离。当属性相关、具有不同值域（不同的方差）、并且数据分布近似高斯正态分布时，欧几里得距离的拓展，Mahalanobis距离是有用的。(公式）
4.选取正确的邻近性度量

对于许多稠密的、连续的数据，通常使用距离度量，如欧几里得距离等。
连续属性之间的邻近度通常用属性值的差表示，并且距离度量提供了一种将这些差组合到总邻近性度量的良好方法。
对于稀疏数据，常常包含非对称的属性，通常使用忽略0-0匹配d的相似度度量。
这反映了一个事实：对于一对复杂对象，相似度依赖于它们共同具有的性质数目，而不是依赖而它们都缺失的数目。在特殊情况下，对于稀疏的、非对称的数据，大部分对象都只具有少量被属性描述的性质，因此如果考虑它们都不具有性质的话，它们都高度相似。
